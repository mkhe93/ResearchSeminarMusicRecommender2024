{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Search for User-Based Neighborhood Algorithm\n",
    "\n",
    "- conduct grid search for a given set of parameters (alpha, beta, neighborhood size, q)\n",
    "- export each parameter setting with the achieved metrics to .txt file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.19 | packaged by conda-forge | (main, Mar 20 2024, 12:38:46) [MSC v.1929 64 bit (AMD64)] on win32\n",
      "Project directory:  C:\\Users\\s8347434\\Documents\\RecSys2024\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# setting proper working directory\n",
    "PROJECT_DIRECTORY = Path(os.path.abspath('')).resolve().parents[0]\n",
    "sys.path.extend([str(PROJECT_DIRECTORY)])\n",
    "\n",
    "print(f'Python {sys.version} on {sys.platform}')\n",
    "print('Project directory: ', PROJECT_DIRECTORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\s8347434\\.conda\\envs\\recsys\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from implicit.evaluation import leave_k_out_split\n",
    "from src.utilities.Helper import load_data, create_sparse_matrix\n",
    "from src.utilities.NeighborAlgorithms import NeighborhoodAlgorithms, NeighborhoodRecommender\n",
    "from src.utilities.Metrics import Evaluation, Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILENAME = PROJECT_DIRECTORY / \"data/processed/user_item_interaction_FILTERED_ANONYMIZED.txt\"\n",
    "DATASET = \"real\"\n",
    "ROWS = 500000\n",
    "TRAIN_TEST_SPLIT_STRATEGY = 42\n",
    "FOLDS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500000, 2)\n",
      "Index(['userID', 'itemID'], dtype='object')\n",
      "Median user interactions:  16.0\n",
      "The new size is:  (451034, 2)\n",
      "Number of (users, items):  (5183, 28081)\n",
      "451034\n",
      "Matrix sparsity: 0.31%\n"
     ]
    }
   ],
   "source": [
    "db_interaction = load_data(FILENAME, rows = ROWS, dataset=DATASET)\n",
    "print(db_interaction.shape)\n",
    "print(db_interaction.keys())\n",
    "\n",
    "if ROWS is not None:\n",
    "    # ONLY FOR SUBSETS: drop users below median interactions\n",
    "    threshold = np.median(db_interaction['userID'].value_counts())\n",
    "    print(\"Median user interactions: \", threshold)\n",
    "    # for manual values to remove\n",
    "    threshold = 20\n",
    "    filter_users = db_interaction['userID'].value_counts() >= threshold\n",
    "    filter_users = filter_users[filter_users].index.tolist()\n",
    "\n",
    "    db_interaction = db_interaction[db_interaction['userID'].isin(filter_users)].reset_index(drop=True)\n",
    "    print(\"The new size is: \", db_interaction.shape)\n",
    "\n",
    "sparse_user_item_interaction, user_index, item_index = create_sparse_matrix(db_interaction, dataset=DATASET)\n",
    "\n",
    "print(\"Number of (users, items): \", sparse_user_item_interaction.shape)\n",
    "\n",
    "print(sparse_user_item_interaction.getnnz())\n",
    "\n",
    "n_total = sparse_user_item_interaction.shape[0]*sparse_user_item_interaction.shape[1]\n",
    "n_ratings = sparse_user_item_interaction.nnz\n",
    "sparsity = n_ratings/n_total\n",
    "print(f\"Matrix sparsity: {round(sparsity*100,2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_set, test_set = train_test_split(sparse_user_item_interaction, user_index, item_index, train_percentage=0.8, k=FOLDS, split_strategy=TRAIN_TEST_SPLIT_STRATEGY)\n",
    "train_set, test_set = leave_k_out_split(sparse_user_item_interaction, K=10, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total downloads per customer: [  97   23  143   42   35  139   21  385   24   45   33   23   23   82\n",
      "   20   56   23   21   35   41   27   56   20   28   43   76   24   25\n",
      "   35   77   37   65  102   97  297  364   70   31  175   47   34   25\n",
      "  240  120   53  468   62   81   38   28   22   41   60   26   23   41\n",
      "   58   32   27   38   41 1150   26   48  199  107   20  122   20   27\n",
      "   39   20   58   43   41   26   20   39   43   91   50   78  120   29\n",
      "   65  119   22   27   42   23   37  125   20   30   30   20   82   22\n",
      "   85  110]\n",
      "Total downloads per customer in train: [  87   13  133   32   25  129   11  375   14   35   23   13   13   72\n",
      "   10   46   13   11   25   31   17   46   10   18   33   66   14   15\n",
      "   25   67   27   55   92   87  287  354   60   21  165   37   24   15\n",
      "  230  110   43  458   52   71   28   18   12   31   50   16   13   31\n",
      "   48   22   17   28   31 1140   16   38  189   97   10  112   10   17\n",
      "   29   10   48   33   31   16   10   29   33   81   40   68  110   19\n",
      "   55  109   12   17   32   13   27  115   10   20   20   10   72   12\n",
      "   75  100]\n",
      "Total downloads per customer in test: [10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10\n",
      " 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10\n",
      " 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10\n",
      " 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10\n",
      " 10 10 10 10]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2)\n",
    "NUMBER_USERS = 10\n",
    "NUMB_EVAL_USERS = 100\n",
    "TOP_N = 10\n",
    "EVAL_USERS = np.random.choice(user_index.cat.categories, NUMB_EVAL_USERS, replace=False)\n",
    "EVAL_USERS_IDX = [user_index.cat.codes[user_index==user].unique()[0] for user in EVAL_USERS]\n",
    "# print(f'CustomerIDs: {EVAL_USERS}')\n",
    "if TRAIN_TEST_SPLIT_STRATEGY == \"cross-fold\":\n",
    "    print(f'Total downloads per customer: {sparse_user_item_interaction[EVAL_USERS_IDX].getnnz(axis=1)}')\n",
    "    for fold in range(FOLDS):\n",
    "        print(f'Total downloads per customer in train: {train_set[fold][EVAL_USERS_IDX].getnnz(axis=1)}')\n",
    "        print(f'Total downloads per customer in test: {test_set[fold][EVAL_USERS_IDX].getnnz(axis=1)}')\n",
    "else:\n",
    "    print(f'Total downloads per customer: {sparse_user_item_interaction[EVAL_USERS_IDX].getnnz(axis=1)}')\n",
    "    print(f'Total downloads per customer in train: {train_set[EVAL_USERS_IDX].getnnz(axis=1)}')\n",
    "    print(f'Total downloads per customer in test: {test_set[EVAL_USERS_IDX].getnnz(axis=1)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Prepare the Algorithms and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_based_topN = NeighborhoodRecommender(NeighborhoodAlgorithms)\n",
    "user_based_topN.add_algorithm('user_based_neighborhood')\n",
    "user_based_iterative_async_topN = NeighborhoodRecommender(NeighborhoodAlgorithms)\n",
    "user_based_iterative_async_topN.add_algorithm('user_based_iterative_asym_neighborhood')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of metrics to apply\n",
    "metrics_list = ['MatchCount', 'Precision', 'MR', 'MRR', 'MAP', 'NDCG','Coverage', 'APLT', 'ARP']\n",
    "\n",
    "# Instantiate the Evaluation class\n",
    "evaluator = Evaluation(Metrics, sparse_user_item_interaction)\n",
    "\n",
    "# Add metrics from the Metrics class\n",
    "for metric in metrics_list:\n",
    "    evaluator.add_metric(metric)\n",
    "\n",
    "def evaluate(evaluators_predictions):# Evaluate metrics for each evaluator and store results\n",
    "    results = []\n",
    "    for evaluator_name, recommendations in evaluators_predictions.items():\n",
    "        result = {\n",
    "            'Evaluator': evaluator_name,\n",
    "            'MatchCount': evaluator.evaluate('MatchCount', recommendations, test_set, user_index, item_index),\n",
    "            'Precision': evaluator.evaluate('Precision', recommendations, test_set, user_index, item_index),\n",
    "            'MR': evaluator.evaluate('MR', recommendations, test_set, user_index, item_index),\n",
    "            'MRR': evaluator.evaluate('MRR', recommendations, test_set, user_index, item_index),\n",
    "            'MAP': evaluator.evaluate('MAP', recommendations, test_set, user_index, item_index),\n",
    "            'NDCG': evaluator.evaluate('NDCG', recommendations, test_set, user_index, item_index),\n",
    "            'Coverage': evaluator.evaluate('Coverage', recommendations, test_set, user_index, item_index),\n",
    "            'APLT': evaluator.evaluate('APLT', recommendations, test_set, user_index, item_index, threshold=0.2),\n",
    "            'ARP': evaluator.evaluate('ARP', recommendations, test_set, user_index, item_index)\n",
    "        }\n",
    "        results.append(result)\n",
    "\n",
    "    # Create a DataFrame from the results\n",
    "    df = pd.DataFrame(results)\n",
    "    #df.set_index('Evaluator', inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list = []\n",
    "results_iterative_asym_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Start the Parameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = [0, 0.2, 0.4, 0.5, 0.6, 0.8, 1]\n",
    "locality = [1, 2, 3, 4, 5, 6]\n",
    "neighborhood_size = [5, 10, 20, 50, 100, 200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = len(alpha) * len(locality) * len(neighborhood_size)\n",
    "i = 1\n",
    "\n",
    "for a in alpha:\n",
    "    for q in locality:\n",
    "        print(f\"Progress: {i/steps *100:.2f}%\")\n",
    "        user_based_topN.fit(user_item_matrix=train_set, alpha=a, q=q)\n",
    "        for n in neighborhood_size:\n",
    "            UserBasedRecoms = user_based_topN.recommend(EVAL_USERS, train_set, user_index, item_index, TOP_N, neighborhood_size=n, already_interacted=[])\n",
    "            evaluators_predictions = {'UserKNN': UserBasedRecoms}\n",
    "            temp_df = evaluate(evaluators_predictions)\n",
    "            temp_df['alpha'] = a\n",
    "            temp_df['q'] = q\n",
    "            temp_df['neighborhood_size'] = n\n",
    "            results_list.append(temp_df)\n",
    "            i += 1\n",
    "print(f\"Progress: {i/steps *100:.2f}%\")\n",
    "results_df = pd.concat(results_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = results_df.reset_index(drop=True)\n",
    "# results_df.to_csv(f\"../data/evaluation/parameter_tuning/user-based-top{TOP_N}-{NUMB_EVAL_USERS}user-leaveK-{ROWS}Rows.txt\", sep=\"\\t\", encoding='utf-16', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = [0, 0.2, 0.4, 0.5, 0.6, 0.8, 1]\n",
    "beta = [0.3, 0.5, 0.7,  0.9, 1, 1.2, 1.4]\n",
    "locality = [1, 2, 3, 4, 5, 6]\n",
    "neighborhood_size = [10, 50, 100, 200, 300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = len(alpha) * len(beta) * len(locality) * len(neighborhood_size)\n",
    "i = 1\n",
    "for a in alpha:\n",
    "    for q in locality:\n",
    "        user_based_iterative_async_topN.fit(user_item_matrix=train_set, alpha=a, q=q)\n",
    "        print(f\"Progress: {i/steps *100:.2f}%\")\n",
    "        for b in beta:\n",
    "            for n in neighborhood_size:\n",
    "                UserBasedAsymRecoms = user_based_iterative_async_topN.recommend(EVAL_USERS, train_set, user_index, item_index, TOP_N, neighborhood_size=n, beta=b)\n",
    "                evaluators_predictions = {'UserIterativeAsymKNN': UserBasedAsymRecoms}\n",
    "                temp_df = evaluate(evaluators_predictions)\n",
    "                temp_df['alpha'] = a\n",
    "                temp_df['q'] = q\n",
    "                temp_df['beta'] = b\n",
    "                temp_df['neighborhood_size'] = n\n",
    "                results_iterative_asym_list.append(temp_df)\n",
    "                i += 1\n",
    "\n",
    "results_iterative_asym_df = pd.concat(results_iterative_asym_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_iterative_asym_df = results_iterative_asym_df.reset_index(drop=True)\n",
    "# results_iterative_asym_df.to_csv(f\"../data/evaluation/parameter_tuning/user-based-iterative-asym-top{TOP_N}-{NUMB_EVAL_USERS}user-leaveK-{ROWS}Rows.txt\", sep=\"\\t\", encoding='utf-16', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = [0, 0.2, 0.4, 0.5, 0.6, 0.8, 1]\n",
    "beta = [0.3, 0.5, 0.7,  0.9, 1, 1.2, 1.4]\n",
    "locality = [1, 2, 3, 4, 5, 6]\n",
    "neighborhood_size = [None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = len(alpha) * len(beta) * len(locality) * len(neighborhood_size)\n",
    "i = 1\n",
    "for a in alpha:\n",
    "    for q in locality:\n",
    "        user_based_iterative_async_topN.fit(user_item_matrix=train_set, alpha=a, q=q)\n",
    "        print(f\"Progress: {i/steps *100:.2f}%\")\n",
    "        for b in beta:\n",
    "            for n in neighborhood_size:\n",
    "                UserBasedAsymRecoms = user_based_iterative_async_topN.recommend(EVAL_USERS, train_set, user_index, item_index, TOP_N, neighborhood_size=n, beta=b)\n",
    "                evaluators_predictions = {'UserIterativeAsymKNN': UserBasedAsymRecoms}\n",
    "                temp_df = evaluate(evaluators_predictions)\n",
    "                temp_df['alpha'] = a\n",
    "                temp_df['q'] = q\n",
    "                temp_df['beta'] = b\n",
    "                temp_df['neighborhood_size'] = n\n",
    "                results_iterative_asym_list.append(temp_df)\n",
    "                i += 1\n",
    "\n",
    "results_iterative_asym_df = pd.concat(results_iterative_asym_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_iterative_asym_df = results_iterative_asym_df.reset_index(drop=True)\n",
    "# results_iterative_asym_df.to_csv(f\"../data/evaluation/parameter_tuning/user-based-iterative-asym-top{TOP_N}-full_neighborhood-{NUMB_EVAL_USERS}user-leaveK-{ROWS}Rows.txt\", sep=\"\\t\", encoding='utf-16', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recsys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
